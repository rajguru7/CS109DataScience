{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyyoo/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import scipy.sparse as sparse\n",
    "import sklearn.metrics as metrics\n",
    "import sys as sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata = pd.read_csv(\"Test_Train_Sets/Hum_test.csv\")\n",
    "traindata = pd.read_csv(\"Test_Train_Sets/Hum_train.csv\")\n",
    "valdata = pd.read_csv(\"Test_Train_Sets/Hum_validate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15053, 18), (1116, 18), (1488, 18))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape, testdata.shape, valdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3043 different books and 7113 different users in traindata\n",
      "643 different books and 372 different users in testdata\n",
      "803 different books and 372 different users in valdata\n"
     ]
    }
   ],
   "source": [
    "print '%d different books and %d different users in traindata' %(np.unique(traindata['Title']).shape[0],np.unique(traindata['ID']).shape[0])\n",
    "print '%d different books and %d different users in testdata' %(np.unique(testdata['Title']).shape[0],np.unique(testdata['ID']).shape[0])\n",
    "print '%d different books and %d different users in valdata' %(np.unique(valdata['Title']).shape[0],np.unique(valdata['ID']).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe that contains only useful informations\n",
    "usefulcol = ['Title','ID','Class','Category','Author','ISBN','Count','Address1','Address2','Publisher']\n",
    "dftrain = traindata[usefulcol]\n",
    "dftest = testdata[usefulcol]\n",
    "dfval = valdata[usefulcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3043 2608 709\n"
     ]
    }
   ],
   "source": [
    "title_train = np.unique(dftrain['Title'].values)\n",
    "author_train = np.unique(dftrain['Author'].values)\n",
    "publisher_train = np.unique(dftrain['Publisher'].values)\n",
    "title_train_dict = {i:t for i,t in enumerate(title_train)}\n",
    "author_train_dict = {i:t for i,t in enumerate(author_train)}\n",
    "publisher_train_dict = {i:t for i,t in enumerate(publisher_train)}\n",
    "print title_train.shape[0],author_train.shape[0],publisher_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def author_to_list(author):\n",
    "    #split the list of authors in a single string into a numpy list of authors. remove brackets, '저', '역', '그림', '/', and etc\n",
    "    #remove translater\n",
    "    if author != author:\n",
    "        newauthorlist = []\n",
    "    else:\n",
    "        split_list = np.array(['/'])\n",
    "        mask = np.array([s in author for s in split_list])\n",
    "        split_list = split_list[mask]\n",
    "        \n",
    "        for s in split_list:\n",
    "            author = re.sub(s,' ..',author)\n",
    "        authorlist = author.split(' ..')\n",
    "        authorlist = np.array(filter(None,authorlist))\n",
    "        \n",
    "        translatormask = np.array([(' 역' in a) or (' 공역' in a) or \n",
    "                                   (' 편역' in a) or (' 감수' in a) or \n",
    "                                   (' 옮김' in a) or (' 등역' in a) or\n",
    "                                   (' 엮음' in a) for a in authorlist])\n",
    "        authorlist = authorlist[~translatormask]\n",
    "        \n",
    "        remove_str_list = np.array(['<','>',' 그림',' 저',' 공저',' 글', ' 등저',' 공편','편'])\n",
    "        newauthorlist = np.array([])\n",
    "        for astr in authorlist:\n",
    "            alist = np.array(astr.split(','))\n",
    "            for i,a in enumerate(alist):\n",
    "                for s in remove_str_list:\n",
    "                    a = re.sub(s,'',a)\n",
    "                alist.flat[i]=a\n",
    "            newauthorlist = np.append(newauthorlist,alist)\n",
    "                \n",
    "    return newauthorlist       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe of all books (no duplicates)\n",
    "dfbook = dftrain[['Author','Title','Publisher','ISBN']]\n",
    "dfbook = dfbook[~dfbook.duplicated()]\n",
    "dfbook = dfbook.reset_index()\n",
    "dfbook.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2605,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_entire_list = np.array([])\n",
    "for a in author_train:\n",
    "    author_entire_list = np.append(author_entire_list,author_to_list(a))\n",
    "author_entire_list = np.unique(author_entire_list)\n",
    "author_entire_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_entire_list_dict = {i:a for i,a in enumerate(author_entire_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make binary author lists: Nbooks X Nauthors matrix\n",
    "#Use author_entire_list for index-author matching\n",
    "Xauthor = np.zeros((dfbook.index.shape[0],author_entire_list.shape[0]))\n",
    "for ind in dfbook.index:\n",
    "    alist = author_to_list(dfbook.iloc[ind]['Author'])\n",
    "    for a in alist:\n",
    "        aind = np.where(author_entire_list==a)[0][0]\n",
    "        Xauthor[ind,aind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Xauthor_Hum.txt',Xauthor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make binary publisher lists: Nbooks X Npublihser matrix\n",
    "#use publisher_train or publisher_train_dict for index matching\n",
    "Xpublisher = np.zeros((dfbook.index.shape[0],publisher_train.shape[0]))\n",
    "for ind in dfbook.index:\n",
    "    pub = dfbook.iloc[ind]['Publisher']\n",
    "    pind = np.where(publisher_train==pub)[0][0]\n",
    "    Xpublisher[ind,pind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Xpublisher_Hum.txt',Xpublisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Make similarity matrix for titles: Nbooks X Nbooks matrix\n",
    "#Use dfbooks for index-title matching\n",
    "Nbooks =dfbook.index.shape[0]\n",
    "simmat_title = np.zeros((Nbooks,Nbooks))\n",
    "for i in range(Nbooks):\n",
    "    simmat_title[i,i] = 1.0\n",
    "    for j in range(i+1,Nbooks):\n",
    "        simmat_title[i,j] = similar(dfbook.iloc[i]['Title'],dfbook.iloc[j]['Title'])\n",
    "        simmat_title[j,i] = simmat_title[i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('simmat_title_Hum.txt',simmat_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simmat_author = Xauthor.dot(Xauthor.T)\n",
    "simmat_publisher = Xpublisher.dot(Xpublisher.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simmat = 0.5*simmat_title+0.4*simmat_author+0.1*simmat_publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_based_on_similarity(dftrain,userid,Nrec,simmat,dfbook=dfbook):\n",
    "    #recommend Nrec books similar to the books that the chosen user has bought so far\n",
    "    #input: \n",
    "    # dftrain: training data\n",
    "    # userid: id of the user\n",
    "    # Nrec: Number of books to be recommended\n",
    "    # dfbook: the dataframes of books\n",
    "    # simmat: Nbooks X Nbooks similarity matrix\n",
    "    usertrans = dftrain[dftrain['ID']==userid]  #user transaction\n",
    "    usertrans = usertrans[['Author','Title','Publisher','ISBN']]\n",
    "    usertrans = usertrans[~usertrans.duplicated()]\n",
    "    excludeind = np.array([])\n",
    "    simvec = np.zeros(simmat.shape[1])\n",
    "    for ind in usertrans.index:\n",
    "        bookind = dfbook[(dfbook['Title']==usertrans[usertrans.index == ind]['Title'].values[0]) \n",
    "                         & (dfbook['Publisher']==usertrans[usertrans.index == ind]['Publisher'].values[0])].index[0]\n",
    "        simvec = simvec+simmat[bookind,:]\n",
    "        excludeind = np.append(excludeind,[bookind])\n",
    "    simvec = simvec/usertrans.index.shape[0]\n",
    "    dfsimvec = dfbook\n",
    "    dfsimvec['Similarity'] = simvec\n",
    "\n",
    "    dfsimvec = dfsimvec.iloc[filter(lambda x: x not in excludeind,dfsimvec.index)]\n",
    "    return dfsimvec.sort('Similarity',ascending=False)[:Nrec]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_based_on_similarity(dftrain,dftest['ID'].values[10],4,simmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_percentage(dftrain,dftest,Nrec,weightdict,simmat_author=simmat_author,\n",
    "                       simmat_title=simmat_title,simmat_publisher=simmat_publisher):\n",
    "    simmat = weightdict['Author']*simmat_author+weightdict['Title']*simmat_title\n",
    "    +weightdict['Publisher']*simmat_publisher\n",
    "    testusers = np.unique(dftest['ID'].values)\n",
    "    totalbooks = 0\n",
    "    totalsuccess = 0\n",
    "    for user in testusers:\n",
    "        dfrec = recommend_based_on_similarity(dftrain,user,Nrec,simmat)\n",
    "        dftestuser = dftest[dftest['ID']==user]\n",
    "        totalbooks = totalbooks+Nrec\n",
    "        for i in range(dfrec.shape[0]):\n",
    "            if np.any((dftestuser['Title']==dfrec.iloc[i]['Title']) \n",
    "                & (dftestuser['Publisher']==dfrec.iloc[i]['Publisher'])):\n",
    "                totalsuccess=totalsuccess+1\n",
    "    return float(totalsuccess)/float(totalbooks)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "weightdict = {'Author':0.4,'Title':0.5,'Publisher':0.1}\n",
    "print predict_percentage(dftrain,dfval,6,weightdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stepsize = 0.05\n",
    "\n",
    "Aws = np.arange(0,1,stepsize)\n",
    "\n",
    "accuracy_list = []\n",
    "for A in Aws:\n",
    "    Tws = np.arange(0,1-A,stepsize)\n",
    "    for T in Tws:\n",
    "        P = 1-A-T\n",
    "        weightdict = {'Author':A,'Publisher':P,'Title':T}\n",
    "        percentage = predict_percentage(dftrain,dfval,6,weightdict)\n",
    "        accuracy = weightdict\n",
    "        accuracy['accuracy'] = percentage\n",
    "        accuracy_list = accuracy_list+[accuracy]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('accuracy_hum.json', 'w') as fp:\n",
    "    json.dump(accuracy_list, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy_list)\n",
    "accuracy_df = accuracy_df.sort('accuracy',ascending=False)\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestweightdict = dict(accuracy_df[accuracy_df['accuracy']==accuracy_df['accuracy'].max()].mean())\n",
    "print bestweightdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Estimate Test error\n",
    "testerror = predict_percentage(dftrain,dftest,4,bestweightdict)\n",
    "print testerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
