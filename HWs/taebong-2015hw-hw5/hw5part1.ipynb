{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Great Lobster! But awful service :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://i.onionstatic.com/avclub/5336/19/16x9/960.jpg\" width=640 height=480/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We continue to look at recommendation data this homework, but through a very different angle. Instead of trying to answer the question \"Which restaurant would I like\", we look at more aggregate behavior, trying to learn from different reviewers what exactly a restaurant is good for.\n",
    "\n",
    "In other words, is the food great? Ambiance Bad? Or are both good? Or bad?\n",
    "\n",
    "So we need to figure out if a given review contains information about the food, the ambiance or both, and if it is a positive or negative review. Note that reviews might cover both topics, so we need to do this analysis on the level of single sentences.\n",
    "\n",
    "We do this at the level of individual sentences, in multiple reviews, about a restaurant.\n",
    "\n",
    "<img src=\"setup.jpg\" width=500 height=300/>\n",
    "\n",
    "What you will carry out here might be termed a first salvo in such an analysis. It will be a fairly complete salvo, but we shall point out many points of improvement along the road, should you want to pursue a project of this kind.\n",
    "\n",
    "There are many aspects of the data science process that we will touch on in this homework, and many skills that you will gain some familiarity with.\n",
    "\n",
    "1. We will use Spark to do a good fraction of our data reduction. This mimics the situation at many companies, where Hadoop/Spark and other such technologies are used to reduce \"big\" data from many different sources such as logfiles, customer data, etc into features which are ready for machine learning and statistical modeling.\n",
    "2. We will use simple Natural Language processing techniques to parse sentences, splitting them into nouns and adjectives\n",
    "3. We will use the unsupervised probabilistic clustering algorithm Latent Dirichlet Allocation, or LDA, to extract topics from the nouns in review text.\n",
    "4. We will use Naive Bayes Machine Learning to carry out sentiment analysis using adjectives from the review text. This kind of analysis is usually done with external sentiment annotated corpus's such as SentiWordNet, but we hope the calculations will give you an idea of what is involved.\n",
    "5. We will obtain topic estimation from the LDA and probability estimates for sentences in the review from the above classifier to answer the question: what does this review have to say about topics concerning restaurants\n",
    "6. We will use somewhat bayesian techniques which fall under the nomenclatures \"Hierarchical Model\" and \"Empirical Bayes\" to regularize the above probability estimates.\n",
    "\n",
    "(image from Annie Hall, from http://www.avclub.com/article/learning-about-love-meatballs-gnocchi-lobsters-and-214971)\n",
    "\n",
    "This homework is split into two parts. You are looking at the first part `hw5part1.ipynb`. You can run this part on your own machine using Spark. (It was developed on a Mac with Homebrew installed Spark (`brew install apache-spark`). You can also work on it inside of your Vagrant virtual Machine or on AWS. \n",
    "\n",
    "Part 2 (`hw5part2.ipynb`) does not need Spark. You can save the data at the end of part 1 and use it in part 2. Of-course if you realize you made a mistake, you will need to go back and run Part 1 again.\n",
    "\n",
    "IF YOU WORK ON AWS **SHUT DOWN** THE AWS CLUSTER WHEN YOU STOP WORKING for a few hours on the HW! If you just leave the cluster open and then work some more the next day, you will find yourself in a world of financial pain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## This homework is due Thursday, November 19th 2015, at 11:59PM EST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll grade the last commit you make before the homework deadline. We will be looking for the files `hw5part1.ipynb`, `hw5part2.ipynb`, and `dftouse.csv`.\n",
    "\n",
    "**Start NOW. This is a long homework.** There is a lot to read, and while there are less computations than on HW4, you are mixing several techniques. Start **NOW** especially to make sure you have Spark working! You might have to try Vagrant, AWS etc to get things running! Labs 8, 9, and 10 will help you. In 9, Bayes2.ipynb has a simple exposition of the Gaussian-Gaussian Bayesian model, while Lab 10 has a worked Naive Bayes example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Table of Contents\n",
    "* [Great Lobster! But awful service :-(](#Great-Lobster!-But-awful-service-:-%28)\n",
    "\t* [This homework is due Thursday, November 19th 2015, at 11:59PM EST.](#This-homework-is-due-Thursday,-November-19th-2015,-at-11:59PM-EST.)\n",
    "\t* [Q1. Using Spark and Natural Language Processing to preprocess the data](#Q1.-Using-Spark-and-Natural-Language-Processing-to-preprocess-the-data)\n",
    "\t\t* [Setting Python packages and Spark up](#Setting-Python-packages-and-Spark-up)\n",
    "\t\t* [Reading in the data](#Reading-in-the-data)\n",
    "\t\t* [Sub-sampling the data](#Sub-sampling-the-data)\n",
    "\t\t\t* [1.1 Create a subsampled-dataframe](#1.1-Create-a-subsampled-dataframe)\n",
    "\t\t* [Creating responses](#Creating-responses)\n",
    "\t\t\t* [1.2 Create a `responses` column to classify reviews as positive or negative](#1.2-Create-a-responses-column-to-classify-reviews-as-positive-or-negative)\n",
    "\t\t* [Natural Language processing](#Natural-Language-processing)\n",
    "\t\t\t* [Parsing language](#Parsing-language)\n",
    "\t\t\t* [1.3 Run `get_parts` on the `text` column of the dataframe](#1.3-Run-get_parts-on-the-text-column-of-the-dataframe)\n",
    "\t* [2. Getting the nouns and doing LDA](#2.-Getting-the-nouns-and-doing-LDA)\n",
    "\t\t* [Why LDA on nouns?](#Why-LDA-on-nouns?)\n",
    "\t\t\t* [Creating the vocabulary](#Creating-the-vocabulary)\n",
    "\t\t\t* [2.1 Create a Corpus in bag of words form](#2.1-Create-a-Corpus-in-bag-of-words-form)\n",
    "\t\t* [Unsupervised topic extraction with LDA](#Unsupervised-topic-extraction-with-LDA)\n",
    "\t\t\t* [2.2 Carry out LDA](#2.2-Carry-out-LDA)\n",
    "\t\t\t* [2.3 Interpret the topics you get with LDA](#2.3-Interpret-the-topics-you-get-with-LDA)\n",
    "\t* [3. Supervised sentiment analysis with Naive Bayes](#3.-Supervised-sentiment-analysis-with-Naive-Bayes)\n",
    "\t\t* [Getting the adjectives](#Getting-the-adjectives)\n",
    "\t\t\t* [3.1 Create the vocabulary of adjectives for the Naive Bayes Classifier.](#3.1-Create-the-vocabulary-of-adjectives-for-the-Naive-Bayes-Classifier.)\n",
    "\t\t* [Get the adjective \"features\" and responses, training and test sets](#Get-the-adjective-\"features\"-and-responses,-training-and-test-sets)\n",
    "\t\t\t* [Transforming to Bag-Of-Words representation](#Transforming-to-Bag-Of-Words-representation)\n",
    "\t\t* [Support code to run a Naive Bayes Classifier.](#Support-code-to-run-a-Naive-Bayes-Classifier.)\n",
    "\t\t\t* [3.2 Write a custom score function, log-likelihood](#3.2-Write-a-custom-score-function,-log-likelihood)\n",
    "\t\t\t* [Cross-Validation](#Cross-Validation)\n",
    "\t\t\t* [Calibration of a classifier](#Calibration-of-a-classifier)\n",
    "\t\t\t* [3.3 Write a custom cross-validation loop](#3.3-Write-a-custom-cross-validation-loop)\n",
    "\t\t\t* [3.4 Test time!](#3.4-Test-time!)\n",
    "\t* [4. Putting topics and sentiment analysis together](#4.-Putting-topics-and-sentiment-analysis-together)\n",
    "\t\t* [Naive Bayes Probabilities](#Naive-Bayes-Probabilities)\n",
    "\t\t\t* [4.1 Obtain log-probabilities for the adjectives](#4.1-Obtain-log-probabilities-for-the-adjectives)\n",
    "\t\t\t* [4.2 Write a function to compute the probability that a sentence is positive.](#4.2-Write-a-function-to-compute-the-probability-that-a-sentence-is-positive.)\n",
    "\t\t\t* [Compute the topic for a sentence](#Compute-the-topic-for-a-sentence)\n",
    "\t\t\t* [Create a dataframe with all of this information](#Create-a-dataframe-with-all-of-this-information)\n",
    "\t\t\t* [4.3 Get the stats](#4.3-Get-the-stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Q1. Using Spark and Natural Language Processing to preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this part of the homework you will use Spark to parse the restaurant reviews and extract nouns and adjectives. We will use this data later to create the topics of the reviews and do a sentiment analysis. \n",
    "\n",
    "You can use Spark here either on your laptop, directly on the mac, on the Vagrant virtual machine on mac or windows, or on AWS. Lab 8 has all the goods, and hopefully you know what to do by now since you have been going to section.\n",
    "\n",
    "NOTE: if you are running into memory problems, try removing calls to `.cache()` in Spark. This will ease some memory pressure, at the cost of taking longer to run.\n",
    "\n",
    "For this part of the homework we will run Spark . You should make sure your virtual machine has at-least 2GB of virtual memory. If you are using AWS, use a medium instance, at the very least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setting Python packages and Spark up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first thing we have to do is setting up spark. This can be complex, so do this REALLY early on, even if you dont have time to do anything else in the homework early.\n",
    "\n",
    "You will need to make sure you have 3 packages installed:\n",
    "\n",
    "- `findspark`: `pip install findspark`.\n",
    "- `gensim`: for LDA. Use conda for this. `conda install gensim`. DO NOT use pip for this. You will find pain\n",
    "- `pattern`: for natural language processing, `pip install pattern`.\n",
    "\n",
    "Notice that below I set the full path to Anaconda python. I need to do this as I usually dont put Anaconda on my path, and thus need to provide the full path in the environment variable `PYSPARK_PYTHON`. If Anaconda is on your path, you probably wont need this. In fact, for some of you it might cause Spark to stop working, so comment it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['PYSPARK_PYTHON'] = '/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next cell sets up Spark for you. Your exact incantation might be different. You might not need `findspark`. This is what I had to do.\n",
    "\n",
    "Make sure you are using Spark 1.5.0+ .  On my Mac, I use the JDK (as opposed to the JRE) from Java 8 and Spark 1.5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start up the Spark context. This is what starts java up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you started Spark from a terminal or the ipython notebook from a terminal, you will see all kinds of logging information on the terminal. This information will tell you if something is wrong.\n",
    "\n",
    "We also want to check if the same Python is running on all the child execution processes on your machine/in the cluster. If you did this right the python version printed below from 10 different executors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]',\n",
       " '3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(range(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ought tomatch the one here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We create a `SQLContext` from our Spark Context so that we can use Spark's dataframe support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The file is available at https://s3.amazonaws.com/cs109-2015/restaurants.jsonl or https://dl.dropboxusercontent.com/u/75194/restaurants.jsonl . Download it to your local machine or onto AWS. You might find the command-line (terminal) programs `wget` or `curl` useful. The file is about 900MB large.\n",
    "\n",
    "We use the SQL Context to read our data file in. The data file is a list of json dictionaries, one per line (thus we gave the file the extension _`.jsonl`_), \n",
    "which each correspond to a row in the dataframe.\n",
    "\n",
    "Copy the file somewhere on your path and read it in like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.json.\n: java.io.IOException: No input paths specified in job\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1113)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:114)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:109)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema$lzycompute(JSONRelation.scala:109)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema(JSONRelation.scala:108)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:636)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:635)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:244)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b12b1115892e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrevdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"restaurants.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrevdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.json.\n: java.io.IOException: No input paths specified in job\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1113)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:114)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:109)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema$lzycompute(JSONRelation.scala:109)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema(JSONRelation.scala:108)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:636)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:635)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:244)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "revdf = sqlsc.read.json(\"restaurants.jsonl\")\n",
    "revdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We look at our data by printing the schema for this table: \n",
    "Spark does this this by sampling some rows for all the columns and seeing the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many reviews do we have? 940600\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- business_avg: double (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- business_review_count: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_avg: double (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_review_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We cache the data to make sure it is only read once from disk\n",
    "revdf.cache()\n",
    "print \"How many reviews do we have?\", revdf.count()\n",
    "revdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now create a dataframe which has only those restaurants that have more than 10 reviews. The reason to do this is that we'll be gaining information from individual reviews. This information can be quite disparate. And thus we want enough reviews to get some statistically significant middle ground.\n",
    "\n",
    "At this point you might object: isn't the whole point of reviews and recommendations that users are different, so that middle ground is hard to find. In the last homework even our baseline models took user bias into account. \n",
    "\n",
    "Here though our hypothesis is different: we assume that **people will use similar adjectives to describe a good experience, no matter what star rating they give it**. So, we will be ok as long as the definition of \"good\", \"excellent\", \"poor\" takes into account the user bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bizcounts=revdf.groupby('business_id').count()\n",
    "bizids = (bizcounts[bizcounts['count'] > 10]\n",
    "          .map(lambda r: r.business_id)\n",
    "          .collect()\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many businesses are we left with?  11417\n"
     ]
    }
   ],
   "source": [
    "print \"How many businesses are we left with? \", len(bizids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that the command below creates a new dataframe as dataframes in Spark are immutable. (Remember, in Pandas too, unless we used `inplace=True` we returned a new dataframe from most operations (except queries which returned views))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "revdf_small=revdf[revdf.business_id.isin(bizids)]#creates new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we count how many reviews we have left in total across all restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total reviews:  905600\n"
     ]
    }
   ],
   "source": [
    "print \"Number of total reviews: \", revdf_small.count()#actually causes the subselect to happen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember RDD operations (Dataframes are just a layer over RDD) are lazy in Spark. So what happens here is that the actual subselection only happens on the `.count()`, at which point the data is cached as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sub-sampling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you might have noticed, there are almost a million records in this dataframe. Doing our analysis on such a large dataframe might bust past the java heap space on our individual machines. \n",
    "\n",
    "My modus operandi is to always start with a small data set. My code initially tends to almost always be wrong: its much easier to deal with this if you have small datasets. I also like to use functions like `take(5)` in Spark: this gives me a quick sanity check of the output to see if it makes any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.1 Create a subsampled-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With this in mind, we'll create a sub-sampled dataframe where we have 8 reviews per restaurant. We create a variable `PER_RESTAURANT_SAMPLES` to store this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PER_RESTAURANT_SAMPLES=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write code that randomly subsamples `PER_RESTAURANT_SAMPLES` reviews for each restaurant. \n",
    "\n",
    "First get a python list of all the review_id's in this smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pairs = revdf_small.map(lambda x: (x.business_id,x.review_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_pairs = pairs.groupByKey()\n",
    "smaller_pairs = grouped_pairs.flatMapValues(lambda x: np.random.choice(list(x),size=PER_RESTAURANT_SAMPLES,replace=False))\n",
    "review_ids_smaller = smaller_pairs.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then use this smaller sample and the Spark method `isin` to create a smaller Spark dataframe `subdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "subdf = revdf_small[revdf_small.review_id.isin(review_ids_smaller)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets cache the dataframe and see how many reviews we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91336"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.cache()\n",
    "subdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should have about 91000 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2 Create a `responses` column to classify reviews as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember how we said that we wanted to take into account the user bias in our measures of whether a review was a \"positive\" review or a \"negative\" review?\n",
    "\n",
    "We can do this by asking if the star rating of a user's review is higher or equal to the user's average rating. This may not be the best choice: I found that users use \"good\" adjectives to describe restaurants below their average rating. Something worth playing with and thinking about later.m\n",
    "\n",
    "Add a new column `responses` to our dataframe (remember this will create a new dataframe in Spark, as opposed to the usual behavior in Pandas, see the docs for `withColumn` method on Spark Dataframes). Assign this new dataframe to the same variable `subdf`.\n",
    "Also store the data from the `responses` column in an extra array `resparray`.\n",
    "\n",
    "The `responses` column will be `true` if the star rating of a user's review is greater than or equal to the user average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[_corrupt_record: string, business_avg: double, business_id: string, business_review_count: double, date: string, review_id: string, stars: bigint, text: string, user_avg: double, user_id: string, user_review_count: double]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "subdf = subdf.withColumn('responses',subdf.stars>=subdf.user_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resparray = subdf.map(lambda x: x.responses).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see what our `subdf` looks like. You should see an additional column `responses` with `true`s and `false`s. (Dont get confused with the lower case trues and falses...its Spark's internal representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+---------------------+----------+--------------------+-----+--------------------+------------------+--------------------+-----------------+---------+\n",
      "|_corrupt_record| business_avg|         business_id|business_review_count|      date|           review_id|stars|                text|          user_avg|             user_id|user_review_count|responses|\n",
      "+---------------+-------------+--------------------+---------------------+----------+--------------------+-----+--------------------+------------------+--------------------+-----------------+---------+\n",
      "|           null|3.91666666667|KayYbHCt-RkbGcPdG...|                 12.0|2011-08-15|2xCds3bp0wM6nHMz3...|    4|Alexion's has bee...|               3.6|4-3IU5uUH90m21TWb...|             20.0|     true|\n",
      "|           null|3.91666666667|KayYbHCt-RkbGcPdG...|                 12.0|2011-12-22|UrukGX1emhSRe2fGd...|    3|Good for cheap dr...|3.7368421052599996|bcwr1bFov3PSa1FiG...|             19.0|    false|\n",
      "|           null|3.91666666667|KayYbHCt-RkbGcPdG...|                 12.0|2013-04-25|2_Ru_ASf75kU303rd...|    4|What a cool bar/r...|             3.125|Sjb5e5-gKoLXueFDM...|              8.0|     true|\n",
      "|           null|3.91666666667|KayYbHCt-RkbGcPdG...|                 12.0|2013-09-17|PUY1KIrW0BY9ENb3b...|    3|Good beer selecti...|               3.0|W_NfPGdpM0286WBDN...|              1.0|     true|\n",
      "|           null|3.91666666667|KayYbHCt-RkbGcPdG...|                 12.0|2014-02-16|0klMyorClST8NYGJq...|    5|Grew up near here...|               5.0|h-A_xNeB_xSbc0psq...|              2.0|     true|\n",
      "+---------------+-------------+--------------------+---------------------+----------+--------------------+-----+--------------------+------------------+--------------------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us calculate the prior probability of having positive samples and store it in `priorp`. We'll also calculate `priorn`, the prior probability of having negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.61878120346851184, 0.38121879653148816)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priorp = np.mean(resparray)\n",
    "priorn = 1 - priorp\n",
    "priorp, priorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Natural Language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our next step is to take the text of the reviews and process this text to extract grammar and semantic meaning from it.\n",
    "\n",
    "This is in general a hard problem. Machine learning models are usually only as good as the features you give them. The principle they follow is: garbage in, garbage out. \n",
    "\n",
    "Parsing text is hard. It used to be that texts in the old days had good grammatical structure, well formed sentences, and so on. With stuff being written online, many rules are broken. So for example I found some reviews with as many as 10 ellipsis dots between words!\n",
    "\n",
    "Still, we shall use some very standard sentence parsing techniques from the python library `pattern` to do our parsing. The reason for this is that our main aim is to extract nouns and adjectives. This is really a first attempt, and if you are doing sentiment analysis or other text analysis on your project you should investigate better, more robust techniques if you need them.\n",
    "\n",
    "We create a list of what we consider punctuation below, and obtain our stopwords from the `sklearn` stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Parsing language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pay attention to this function as it provides an example of the kind of cleaning that needs to be done with text. We dont do a particularly sophisticated job here, but its simple and illustrative for the purposes of a homework. You will want to do a more thorough job on your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above we compile two regular expressions we will use. The first matched two or more dots in an ellipsis, while the second matches too or more dashes. We'll assume a single dot is a sentence separator while an single dash means a hyphenated word which is important to keep.\n",
    "\n",
    "Note that it is important for us to preserve sentence structure here as we intend to utilize it to extract nouns and adjectives. It might even be better to compare against lists of known nouns and adverbs, but then one might miss grammatical structure in which nouns are used like adjectives and vice versa. Its all worth playing with.\n",
    "\n",
    "The function `get_parts` below takes in an input review and returns a tuple of nouns and adjectives. Each member of the tuple is a list of lists. For instance the first member is a list, whose members are lists of nouns. Each such member is mined from a sentence. Some sentences will not have any representation as they dont pass the battery of conditions we impose below. Indeed if any sentence has no adjectives or no nouns we remove it. The idea eventually is to treat each such sentence as a review, with the nouns used to find the topic (using LDA), and the adjectives used to do sentiment analysis (via Naive Bayes).\n",
    "\n",
    "`get_parts` does the following:\n",
    "\n",
    "- First we substitute anything more than two dots or two dashes by a space so the letters on either side are treated as separate words\n",
    "- Then we use pattern to parse the text into sentences. The sentences are tokenized into words, and lemmatized, which means that we convert words into their basic form, for [example](https://github.com/piskvorky/topic_modeling_tutorial/blob/master/1%20-%20Streamed%20Corpora.ipynb):\n",
    "\n",
    "> \"work\", \"working\", \"works\", \"worked\", \"working\" => same lemma: \"work\"\n",
    "\n",
    "- The words are associated with tags from the [Penn Treebank](http://www.clips.ua.ac.be/pages/mbsp-tags) in the parse process. This enables us to identify parts-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The/DT/B-NP/O/the world/NN/I-NP/O/world is/VBZ/B-VP/O/be the/DT/B-NP/O/the craziest/JJ/I-NP/O/craziest place/NN/I-NP/O/place ././O/O/.\\nI/PRP/B-NP/O/i am/VBP/B-VP/O/be working/VBG/I-VP/O/work hard/RB/B-ADVP/O/hard ././O/O/.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"The world is the craziest place. I am working hard.\", tokenize=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "See the page linked above to interpret what the various annotations mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- We then match for adjectives and nouns separately. While doing this we make sure that the word in question is not a stopword, does not start or end with punctuation, or is not a single letter word.\n",
    "- This gives us pairs of lists of nouns and adjectives from each sentence. If any one of these lists in the pair is empty, we throw the sentence away, because we dont have enough information to process the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is a diagram that shows the process.\n",
    "\n",
    "<img src=\"nlp.jpg\" width=800 height=500/>\n",
    "\n",
    "Here is a brief toy example to demonstrate how `get_parts` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[u'patio', u'job'], [u'lunch', u'egg']], [[u'perfect'], [u'good', u'great']])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parts(\"Have had many other items and just love the food. The patio...job was and...perfect. Lunch is good, and the only egg is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.3 Run `get_parts` on the `text` column of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use Spark's `map` to run this function on the contents of the text column of every row of the dataframe. Store the lazy RDD in `review_parts`. (This should be one line of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "review_parts = subdf.map(lambda x: get_parts(x.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll `take` the first three to see what our code looks like. This way we dont have to wait for everything to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'time',\n",
       "    u'year',\n",
       "    u'summer',\n",
       "    u'break',\n",
       "    u'college',\n",
       "    u'working',\n",
       "    u'ice',\n",
       "    u'cream',\n",
       "    u'factory',\n",
       "    u'hill'],\n",
       "   [u'year', u'memory', u'customer'],\n",
       "   [u'sandwiches', u'pittsburgh-style', u'sort', u'today', u'time', u'place'],\n",
       "   [u'place', u'town'],\n",
       "   [u'sandwich', u'hour'],\n",
       "   [u'thing', u'place', u'home']],\n",
       "  [[u'old'],\n",
       "   [u'fond'],\n",
       "   [u'perfect', u'ancient'],\n",
       "   [u'clean', u'peaceful', u'little', u'oldest'],\n",
       "   [u'pretty', u'late'],\n",
       "   [u'good', u'closer']]),\n",
       " ([[u'drink', u'wing', u'special'],\n",
       "   [u'inside', u'bartender', u'job'],\n",
       "   [u'selection', u'alcohol'],\n",
       "   [u'wing', u'salad'],\n",
       "   [u'gluten', u'place', u'lot', u'food', u'sandwich'],\n",
       "   [u'lunch']],\n",
       "  [[u'cheap', u'daily', u'nice'],\n",
       "   [u'like', u'outside', u'friendly', u'good'],\n",
       "   [u'huge'],\n",
       "   [u'good'],\n",
       "   [u'free', u'fried'],\n",
       "   [u'quick']]),\n",
       " ([[u'service', u'price', u'restroom'],\n",
       "   [u'buffalo', u'chicken', u'sandwich'],\n",
       "   [u'menu', u'bar', u'food', u'food', u'item', u'menu'],\n",
       "   [u'bar', u'path', u'worth', u'trip']],\n",
       "  [[u'great', u'clean'],\n",
       "   [u'delicious'],\n",
       "   [u'typical', u'different'],\n",
       "   [u'cool', u'beaten']])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_parts.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use `collect` to collect in a python list all of our results. This is where the computation actually takes place. This function took me between 20-30 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 2.81 s, total: 9.83 s\n",
      "Wall time: 28min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parseout=review_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Getting the nouns and doing LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to find out if a particular sentence of a review talks about the food quality or the decorations, the ambiance, the service, etc. We look at the nouns of the sentence to do this. The nouns are the first elements of all the tuples we created. Remember we get a list of lists of lists from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'time',\n",
       "   u'year',\n",
       "   u'summer',\n",
       "   u'break',\n",
       "   u'college',\n",
       "   u'working',\n",
       "   u'ice',\n",
       "   u'cream',\n",
       "   u'factory',\n",
       "   u'hill'],\n",
       "  [u'year', u'memory', u'customer'],\n",
       "  [u'sandwiches', u'pittsburgh-style', u'sort', u'today', u'time', u'place'],\n",
       "  [u'place', u'town'],\n",
       "  [u'sandwich', u'hour'],\n",
       "  [u'thing', u'place', u'home']],\n",
       " [[u'drink', u'wing', u'special'],\n",
       "  [u'inside', u'bartender', u'job'],\n",
       "  [u'selection', u'alcohol'],\n",
       "  [u'wing', u'salad'],\n",
       "  [u'gluten', u'place', u'lot', u'food', u'sandwich'],\n",
       "  [u'lunch']],\n",
       " [[u'service', u'price', u'restroom'],\n",
       "  [u'buffalo', u'chicken', u'sandwich'],\n",
       "  [u'menu', u'bar', u'food', u'food', u'item', u'menu'],\n",
       "  [u'bar', u'path', u'worth', u'trip']]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e[0] for e in parseout[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We flatten the outer list which holds all the lists corresponding to one given review out, because we now want to treat each accepted sentence in a review as a separate mini-review. Why do we want to do this for the nouns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Why LDA on nouns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We do LDA only on nouns because the thematic signal is clearer, not-adulterated by other parts of speech.\n",
    "\n",
    "But why do we do it at the granularity of the nouns in a sentence?\n",
    "\n",
    "Think about this from the perspective of clustering. The generative model for LDA is one in which a document comes from a mixture of topics. Each topic can be thought of as a cluster. We need some data which has very clear and strong cluster membership to clearly delineate the clusters. Thus we need to treat the text at a granularity where at-least some sentences have strong cluster membership. For a review, for example, some sentences may talk only about food, and some sentences may talk only about service, and its important to have these so that the topics (clusters) can be clearly established. Thus a sentence is a reasonable granularity, while a paragraph may be too big.\n",
    "\n",
    "We use Spark's `flatMap` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'time',\n",
       "  u'year',\n",
       "  u'summer',\n",
       "  u'break',\n",
       "  u'college',\n",
       "  u'working',\n",
       "  u'ice',\n",
       "  u'cream',\n",
       "  u'factory',\n",
       "  u'hill'],\n",
       " [u'year', u'memory', u'customer'],\n",
       " [u'sandwiches', u'pittsburgh-style', u'sort', u'today', u'time', u'place'],\n",
       " [u'place', u'town'],\n",
       " [u'sandwich', u'hour']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd=sc.parallelize([ele[0] for ele in parseout]).flatMap(lambda l: l)\n",
    "ldadatardd.cache()\n",
    "ldadatardd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you have a set of \"documents\" for the LDA, each corresponding to the nouns in a sentence in all the reviews, lets go about compiling the complete vocabulary that these documents use. We need to do this so that we can set up the documents in the form of a corpus, where each document is a sparse vector of the size of the vocabulary, with a number in the slot for the word indicating how many times that word appeared in a document.\n",
    "\n",
    "So the first thing we want to do is to collect all the words. One more flattening does the job but will have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'time', u'year', u'summer', u'break', u'college']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd.flatMap(lambda word: word).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, in the usual word-count style we reduce to get the word numbers and then just drop these numbers as we dont care for them. We `zipWithIndex` the words to give them an index which we'll use as the place index in the corpus vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabtups = (ldadatardd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use Spark's `collectAsMap` to output the vocabulary `vocab` with the words as keys. We also use `map` to invert this dictionary and get `id2word` which maps the index to the word. This dictionary is then used by Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab=vocabtups.collectAsMap()\n",
    "id2word=vocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'washugyu', u'originality', 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0], vocab.keys()[5], vocab[vocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The size of our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28994"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1 Create a Corpus in bag of words form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Its now your job to create the lazy RDD `documents`.  By lazy we mean that `collect` still needs to be called on it. `documents`  will be a list of lists that reflects the entire set of sentences. Each one of the member lists consists of a set of tuples `(index, count)` where `index` is the index of the corresponding word in the vocabulary and `count` is the number of times it occured in the sentence.\n",
    "\n",
    "`documents` will look something like:\n",
    "\n",
    "```\n",
    "[[(5912, 1), (3809, 1), (14131, 1), (3876, 1)],\n",
    "[(3266, 1), (3652, 1), (11644, 1), (2296, 1), (27516, 1), (8382, 1)],\n",
    " [(17217, 1), (22979, 1), (11210, 1), (18736, 1), (3893, 1), (21307, 1)],\n",
    " ...,\n",
    " [(23980, 1), (24730, 1), (22979, 1), (20012, 1), (11206, 2)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hint: `defaultdict` from the `collections` module might be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "import collections\n",
    "\n",
    "def wordcounter(r,vocab=vocab):\n",
    "    d = collections.defaultdict(int)\n",
    "    for word in r:\n",
    "        d[vocab[word]] += 1\n",
    "    return d.items()\n",
    "\n",
    "documents = ldadatardd.map(wordcounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We take the first 5 to make sure we got the structure right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(23553, 1),\n",
       "  (22434, 1),\n",
       "  (22083, 1),\n",
       "  (27720, 1),\n",
       "  (494, 1),\n",
       "  (20531, 1),\n",
       "  (824, 1),\n",
       "  (6233, 1),\n",
       "  (2267, 1),\n",
       "  (12989, 1)],\n",
       " [(2267, 1), (27629, 1), (1319, 1)],\n",
       " [(22083, 1), (20426, 1), (26617, 1), (28249, 1), (28925, 1), (5694, 1)],\n",
       " [(20426, 1), (11754, 1)],\n",
       " [(28145, 1), (27575, 1)]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And then we `collect` and store in the variable `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Unsupervised topic extraction with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now run the LDA algorithm on the nouns only. The nouns typically reflect the themes of a review sentence and including other parts of speech adds noise. \n",
    "\n",
    "We'll look for two topics. The thinking is that one topic will pick out the food, while the other one will pick up other stuff about restaurants, such as service or decorations. Of course there will be some mixing and such. The decision is rarely ever completely clean.\n",
    "\n",
    "Remember that LDA is an unsupervised algorithm, so there is no \"performance\" to measure. But we would like the two clusters we are looking for to correspond to our intuition that one should cover the food and one the other qualities of the restaurant. Or something similar.\n",
    "\n",
    "But again remember that there is no reason that this intuition is correct. Just like in the recommender models from the last homework, these clusters are \"latent\" factors. But if they do coincide with topics/classes/clusters in our data, LDA is more useful.\n",
    "\n",
    "We run LDA using a method called \"variational inference\", which can be updated online. We run it in chunks of 20,000 documents, hoping that topics have converged by then. This makes subsequent training faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2 Carry out LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You will run `gensim.models.ldamodel.LdaModel` on our corpus, setting the `id2word` argument to our `id2word` dictionary. Since we are trying to find two clusters, set `num_topics` to 2. The model is stored in the variable `lda2` (for LDA with two topics).\n",
    "\n",
    "This algorithm, as mentioned above, is online. More information about it can be found [here]( https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation) and at the other `gensim` docs (https://radimrehurek.com/gensim/models/ldamodel.html). Update it in chunks of 20,000 docs (`update_every=1` and `chunksize=20000`), with `passes=1` over the entire corpus\n",
    "\n",
    "The code takes about 2-3 minutes to run on my machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "lda2 = gensim.models.ldamodel.LdaModel(corpus=corpus,num_topics=2,id2word=id2word,\n",
    "                                       update_every=1,chunksize=20000,passes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.3 Interpret the topics you get with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets print the topics that the LDA gives us. Notice the two topics are combinations of representative words in the way gensim prints them. You can change print options if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.068*food + 0.055*place + 0.031*service + 0.029*time + 0.025*restaurant + 0.016*staff + 0.016*order + 0.015*price + 0.013*table + 0.013*person',\n",
       " u'0.021*chicken + 0.019*sauce + 0.018*pizza + 0.017*burger + 0.015*cheese + 0.014*flavor + 0.014*menu + 0.013*salad + 0.012*fry + 0.012*sandwich']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We print out some \"documents\" or \"sentences\" from the corpus. The variable we use for iteration is called `bow` for bag-of-words. We use `id2word` to print out the words as well...\n",
    "\n",
    "`get_document_topics(bow)` gives us the two clusters we are looking for, and the probability that the sentence referred to one of the topics or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23553, 1), (22434, 1), (22083, 1), (27720, 1), (494, 1), (20531, 1), (824, 1), (6233, 1), (2267, 1), (12989, 1)]\n",
      "[(0, 0.67732991247696173), (1, 0.32267008752303822)]\n",
      "break summer time factory working college cream hill year ice\n",
      "==========================================\n",
      "[(12641, 1), (8548, 1), (1694, 1), (27527, 1)]\n",
      "[(0, 0.89948113920110973), (1, 0.10051886079889016)]\n",
      "path bar worth trip\n",
      "==========================================\n",
      "[(28145, 1), (4186, 1), (5555, 1), (24837, 1)]\n",
      "[(0, 0.105138216995272), (1, 0.89486178300472796)]\n",
      "sandwich companion dinner salad\n",
      "==========================================\n",
      "[(16419, 1), (24837, 1), (18631, 1), (27965, 1), (28145, 1), (23645, 1)]\n",
      "[(0, 0.071641677673213375), (1, 0.92835832232678661)]\n",
      "tuna salad cheese slice sandwich toast\n",
      "==========================================\n",
      "[(26219, 1)]\n",
      "[(0, 0.25008827862498195), (1, 0.74991172137501805)]\n",
      "burger\n",
      "==========================================\n",
      "[(1794, 2), (9250, 1)]\n",
      "[(0, 0.4700970324318568), (1, 0.52990296756814326)]\n",
      "coffee diner\n",
      "==========================================\n",
      "[(20426, 1), (14230, 1)]\n",
      "[(0, 0.83166723815200583), (1, 0.16833276184799417)]\n",
      "place table\n",
      "==========================================\n",
      "[(320, 1), (17297, 1), (4691, 1), (25812, 1), (19743, 1)]\n",
      "[(0, 0.083433866030450429), (1, 0.91656613396954956)]\n",
      "ham banana delish soup pepper\n",
      "==========================================\n",
      "[(12561, 1), (22813, 1), (8766, 1), (25959, 1)]\n",
      "[(0, 0.1000142258865715), (1, 0.8999857741134285)]\n",
      "bun patty onion cheeseburger\n",
      "==========================================\n",
      "[(10862, 1), (8797, 1), (916, 1), (3790, 1), (27575, 1), (14621, 1)]\n",
      "[(0, 0.82238721528269065), (1, 0.17761278471730929)]\n",
      "surprise sister door afternoon hour entry\n",
      "==========================================\n",
      "[(16752, 1), (23531, 1), (444, 1)]\n",
      "[(0, 0.87246097902049402), (1, 0.12753902097950603)]\n",
      "car order minute\n",
      "==========================================\n",
      "[(25601, 1), (3284, 1), (2031, 1)]\n",
      "[(0, 0.62330379288427262), (1, 0.37669620711572738)]\n",
      "shocker party night\n",
      "==========================================\n",
      "[(1177, 1), (21419, 1), (19316, 1), (18575, 1)]\n",
      "[(0, 0.8623412489578357), (1, 0.13765875104216438)]\n",
      "chorus piano lesson lyric\n",
      "==========================================\n",
      "[(4449, 1), (28836, 1), (7334, 1), (8796, 1), (7244, 1), (19277, 1), (25486, 1), (2640, 1), (8337, 2), (28916, 1), (3128, 1), (28252, 1)]\n",
      "[(0, 0.96181980797370525), (1, 0.038180192026294676)]\n",
      "repeating whore idea stage theme mirror butt phrase twinkle slut bitch pianist\n",
      "==========================================\n",
      "[(23560, 1), (2570, 1), (17455, 1), (24269, 2), (4125, 1)]\n",
      "[(0, 0.9280073161415423), (1, 0.071992683858457654)]\n",
      "cornhole month game area screen\n",
      "==========================================\n",
      "[(9161, 1), (23722, 1), (9531, 1), (2148, 1)]\n",
      "[(0, 0.87514473101502721), (1, 0.12485526898497279)]\n",
      "voice raspy friend guy\n",
      "==========================================\n",
      "[(27657, 1), (9935, 1)]\n",
      "[(0, 0.53841679175223989), (1, 0.46158320824776006)]\n",
      "downfall food\n",
      "==========================================\n",
      "[(12058, 1), (23660, 1), (28461, 1), (406, 1)]\n",
      "[(0, 0.11775776266481272), (1, 0.88224223733518736)]\n",
      "mom steak friedness roll\n",
      "==========================================\n",
      "[(26404, 1), (28741, 1), (6278, 1), (6640, 1), (214, 1), (15749, 1)]\n",
      "[(0, 0.080351732421402061), (1, 0.91964826757859797)]\n",
      "build chicken kind taco plate fixing\n",
      "==========================================\n",
      "[(23488, 1), (6242, 1), (28357, 1)]\n",
      "[(0, 0.87484609320143658), (1, 0.12515390679856339)]\n",
      "bartender guest behavior\n",
      "==========================================\n",
      "[(17406, 1)]\n",
      "[(0, 0.74791954961572404), (1, 0.2520804503842759)]\n",
      "look\n",
      "==========================================\n",
      "[(20739, 1)]\n",
      "[(0, 0.25029993715937732), (1, 0.74970006284062274)]\n",
      "ale\n",
      "==========================================\n",
      "[(12905, 2), (6039, 1)]\n",
      "[(0, 0.13265821757933044), (1, 0.86734178242066962)]\n",
      "beer taste\n",
      "==========================================\n",
      "[(1906, 1), (28741, 1), (10862, 1)]\n",
      "[(0, 0.12616396963320931), (1, 0.87383603036679069)]\n",
      "flauta chicken surprise\n",
      "==========================================\n",
      "[(898, 1)]\n",
      "[(0, 0.74759173803345491), (1, 0.25240826196654514)]\n",
      "experience\n",
      "==========================================\n",
      "[(28290, 1)]\n",
      "[(0, 0.35390093997268396), (1, 0.64609906002731599)]\n",
      "hit\n",
      "==========================================\n",
      "[(9935, 1)]\n",
      "[(0, 0.74980869012827445), (1, 0.25019130987172555)]\n",
      "food\n",
      "==========================================\n",
      "[(23488, 1), (7240, 1), (2003, 1), (20436, 1), (16111, 2)]\n",
      "[(0, 0.73511952113765733), (1, 0.26488047886234262)]\n",
      "bartender menu client array drink\n",
      "==========================================\n",
      "[(20745, 1), (11348, 1), (8766, 1)]\n",
      "[(0, 0.12519918211888117), (1, 0.87480081788111885)]\n",
      "dish pizza onion\n",
      "==========================================\n",
      "[(7590, 1), (2031, 1)]\n",
      "[(0, 0.61934688826918383), (1, 0.38065311173081623)]\n",
      "course night\n",
      "==========================================\n",
      "[(16752, 1), (23899, 1), (9531, 1), (11348, 1), (20855, 1)]\n",
      "[(0, 0.65310535780558754), (1, 0.34689464219441246)]\n",
      "car trouble friend pizza style\n",
      "==========================================\n",
      "[(11348, 1), (15889, 1), (6100, 1), (27965, 1), (22601, 1)]\n",
      "[(0, 0.097170345118647569), (1, 0.90282965488135247)]\n",
      "pizza tomato sun slice sweetness\n",
      "==========================================\n",
      "[(11348, 1), (18182, 1)]\n",
      "[(0, 0.1670183122396571), (1, 0.83298168776034298)]\n",
      "pizza mini\n",
      "==========================================\n",
      "[(19609, 1), (22083, 1), (148, 1), (21373, 1)]\n",
      "[(0, 0.75124046991262994), (1, 0.24875953008737009)]\n",
      "staff time service shelf\n",
      "==========================================\n",
      "[(20681, 1), (4205, 1), (28741, 1)]\n",
      "[(0, 0.12508399068600712), (1, 0.87491600931399294)]\n",
      "shrimp flavor chicken\n",
      "==========================================\n",
      "[(20426, 1), (5451, 1), (6060, 1), (11998, 1)]\n",
      "[(0, 0.89354348400634953), (1, 0.10645651599365055)]\n",
      "place news wifi panera\n",
      "==========================================\n",
      "[(19587, 1), (15684, 1), (28741, 1), (26571, 1), (6039, 1), (24837, 3)]\n",
      "[(0, 0.060484263078954964), (1, 0.93951573692104495)]\n",
      "mix spring chicken use taste salad\n",
      "==========================================\n",
      "[(16353, 1), (23531, 1)]\n",
      "[(0, 0.82820222560997769), (1, 0.17179777439002231)]\n",
      "waitress order\n",
      "==========================================\n",
      "[(8548, 1), (24837, 1), (20426, 1), (20919, 1), (25812, 1), (11031, 1)]\n",
      "[(0, 0.60967690570767152), (1, 0.39032309429232848)]\n",
      "bar salad place gourmet soup shame\n",
      "==========================================\n",
      "[(18326, 1)]\n",
      "[(0, 0.61896852866349961), (1, 0.38103147133650045)]\n",
      "kid\n",
      "==========================================\n",
      "[(20426, 1), (2867, 1)]\n",
      "[(0, 0.82707916787176161), (1, 0.1729208321282385)]\n",
      "place reviewer\n",
      "==========================================\n",
      "[(4513, 1), (26538, 1)]\n",
      "[(0, 0.79751733174239681), (1, 0.20248266825760322)]\n",
      "charm bit\n",
      "==========================================\n",
      "[(25900, 1)]\n",
      "[(0, 0.25002898806827017), (1, 0.74997101193172977)]\n",
      "pepperoni\n",
      "==========================================\n",
      "[(21656, 1), (18441, 1), (23531, 1), (23645, 1), (24414, 1)]\n",
      "[(0, 0.1291123108047163), (1, 0.8708876891952837)]\n",
      "bacon potato order toast egg\n",
      "==========================================\n",
      "[(25436, 1), (2530, 1), (25548, 1), (12250, 1), (20270, 1)]\n",
      "[(0, 0.083406218419569092), (1, 0.91659378158043092)]\n",
      "spinach omelet mushroom feta sourness\n",
      "==========================================\n",
      "[(3336, 1), (271, 1)]\n",
      "[(0, 0.23917580683657189), (1, 0.7608241931634282)]\n",
      "home fry\n",
      "==========================================\n",
      "[(9377, 1), (15796, 1), (2828, 1), (5694, 1), (3991, 1)]\n",
      "[(0, 0.91529204535246989), (1, 0.084707954647530101)]\n",
      "change try pace today lunch\n",
      "==========================================\n",
      "[(16161, 1), (5065, 1), (18631, 1), (24837, 1), (9469, 1)]\n",
      "[(0, 0.083353948180538637), (1, 0.91664605181946135)]\n",
      "pear vinaigrette cheese salad goat\n",
      "==========================================\n",
      "[(6819, 1), (25188, 1), (28317, 1)]\n",
      "[(0, 0.14586952289862895), (1, 0.85413047710137102)]\n",
      "piece brick bark\n",
      "==========================================\n",
      "[(18697, 1), (20426, 1), (271, 2), (1297, 1), (15796, 1), (5787, 1), (21674, 1)]\n",
      "[(0, 0.38752318437295197), (1, 0.61247681562704814)]\n",
      "corner place fry mussel try yum herb\n",
      "==========================================\n",
      "[(20426, 1)]\n",
      "[(0, 0.74986160421415904), (1, 0.25013839578584096)]\n",
      "place\n",
      "==========================================\n",
      "[(17378, 1), (13000, 1), (14058, 1), (9003, 1), (23310, 1), (2674, 1), (17427, 1), (19130, 1)]\n",
      "[(0, 0.064099328920601911), (1, 0.93590067107939801)]\n",
      "eggplant corn carrot vegetable broccoli szechuan assortment zucchini\n",
      "==========================================\n",
      "[(28219, 1)]\n",
      "[(0, 0.25092432660824748), (1, 0.74907567339175252)]\n",
      "option\n",
      "==========================================\n",
      "[(20737, 1)]\n",
      "[(0, 0.74944234778163943), (1, 0.25055765221836057)]\n",
      "chinese\n",
      "==========================================\n",
      "[(3336, 1)]\n",
      "[(0, 0.67462084852867266), (1, 0.32537915147132734)]\n",
      "home\n",
      "==========================================\n",
      "[(20426, 1), (7199, 1), (15485, 1), (5694, 1), (9935, 1)]\n",
      "[(0, 0.91561230774270286), (1, 0.084387692257297101)]\n",
      "place person belief today food\n",
      "==========================================\n",
      "[(898, 1), (24667, 1)]\n",
      "[(0, 0.83178819219914546), (1, 0.16821180780085446)]\n",
      "experience eating\n",
      "==========================================\n",
      "[(6081, 1), (25812, 1), (7240, 1), (23534, 1), (785, 1), (24500, 1), (12718, 1), (3991, 1), (9556, 1), (3867, 1), (26325, 1)]\n",
      "[(0, 0.17594901049354691), (1, 0.82405098950645306)]\n",
      "list soup menu muffin paninis made-to-order croissant lunch breakfast round good\n",
      "==========================================\n",
      "[(16753, 1), (22165, 1)]\n",
      "[(0, 0.1787916453553807), (1, 0.82120835464461939)]\n",
      "cap latte\n",
      "==========================================\n",
      "[(15032, 1), (20426, 1), (28219, 1)]\n",
      "[(0, 0.37659454421094823), (1, 0.62340545578905171)]\n",
      "bite place option\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:900:15]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Interpret the topic 0 and the topic 1 in the cell below by looking at the probabilities above. Play with changing the stride and limits in the for loop above to get a clear idea of what Topic 0 and Topic 1 correspond to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*your answer here* <br>\n",
    "Topic 1 corresponds to all foods except sandwich and pizza. Topic 2 corresponds to place and service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Supervised sentiment analysis with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have found the \"topics\" of sentences in all our reviews, we turn our attention to the adjectives in the reviews. \n",
    "\n",
    "Remember that we created a \"responses\" column earlier? What we'll now do is combine all the adjectives in an *entire* review together and use these for sentiment analysis.\n",
    "\n",
    "Typically, such sentiment analysis is done using an external data set such as SentiWordNet. Indeed, such an external analysis typically will perform better, as out positive-negative (1-0) signal is a bit diffuse given the method we used to create it. However, we are trying to learn the techniques here, so it wont hurt us to use our own data set to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Getting the adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we finished the topic modeling, we go on to do the sentiment analysis of the reviews. The first thing to do is to get the adjectives together. We can create a RDD in the same way that we did for the LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'old'],\n",
       "  [u'fond'],\n",
       "  [u'perfect', u'ancient'],\n",
       "  [u'clean', u'peaceful', u'little', u'oldest'],\n",
       "  [u'pretty', u'late'],\n",
       "  [u'good', u'closer']],\n",
       " [[u'cheap', u'daily', u'nice'],\n",
       "  [u'like', u'outside', u'friendly', u'good'],\n",
       "  [u'huge'],\n",
       "  [u'good'],\n",
       "  [u'free', u'fried'],\n",
       "  [u'quick']],\n",
       " [[u'great', u'clean'],\n",
       "  [u'delicious'],\n",
       "  [u'typical', u'different'],\n",
       "  [u'cool', u'beaten']]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdatardd=sc.parallelize([ele[1] for ele in parseout])\n",
    "nbdatardd.cache()\n",
    "nbdatardd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.1 Create the vocabulary of adjectives for the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In analogy with the case of the nouns, create a vocabulary for the adjectives, with the unique adjectives and an index. Store in `adjvocab` the dictionary with the key the adjective and the value the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "adjvocabtups = (nbdatardd.flatMap(lambda l:l).flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()).cache()\n",
    "\n",
    "adjvocab = adjvocabtups.collectAsMap()\n",
    "adid2word=adjvocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5251"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjvocab['delicious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19941"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get the adjective \"features\" and responses, training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There is a slight difference in how we want to deal with the adjective data compared to how we handled the nouns earlier. We want to flatten the list of sentences in a review down to a list of all the adjectives in the review. That is, instead of a list of nouns per sentence of the review, we create a single list of all adjectives for the whole review, summarizing over all its sentences. Then we'll just join these adjectives together into a fake document using whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "Xarraypre=nbdatardd.map(lambda l: \" \".join(list(itertools.chain.from_iterable(l))))\n",
    "Xarray=Xarraypre.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you did these right, the length of these two arrays below should be equal. And any element of the array ought to be a whitespace separated list of adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91336 91336\n",
      "old fond perfect ancient clean peaceful little oldest pretty late good closer\n"
     ]
    }
   ],
   "source": [
    "print  len(Xarray), len(resparray)\n",
    "print Xarray[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we are ready to do train our classifier. As usual, we create a mask to split things into a test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(len(Xarray)), train_size=0.7)\n",
    "mask=np.ones(len(Xarray), dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since `Xarray` and `resparray` are regular python arrays output by Spark, we convert them into numpy arrays for use with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X=np.array(Xarray)\n",
    "y=np.array(resparray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Transforming to Bag-Of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use a vectorizer to transform the space separated adjectives we have to a bag-of-words representation. For the case of the nouns we wrote this vectorizer ourselves. For the adjectives we'll use a vectorizer from `sklearn` to do the jon (see lab 10 or hw3, cs109 2013 for examples, and the `CountVectorizer` documentation from `sklearn`). \n",
    "\n",
    "Here is a function which takes a `X-col` of word based documents and uses the vectorizer `vectorizer` to transform them into a feature matrix where the features are the vocabulary. We'll see how to do this in detail in a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_xy(X_col, y_col, vectorizer):\n",
    "    X = vectorizer.fit_transform(X_col)\n",
    "    y = y_col\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Support code to run a Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do you remember how, in homework 3 we had an asymmetric classifier and dealt with it by creating ROC curves and picking the threshold on them?\n",
    "\n",
    "The approach was a good one, but we could have improved on it in two ways\n",
    "\n",
    "(a) we could have computed ROC curves on validation folds so that we could have error-bars on the ROC curves, rather than eating up the test set to do model comparison.\n",
    "\n",
    "(b) we had the ability to plug in a different score. We could have directly put in a cost, or another such score in the cross-validation process. Some of the theory behind this is covered in the Bayes 3 lab of lab 9. \n",
    "\n",
    "Here we do have a somewhat asymmetric set. So we'll use a different score, designed to maximize probability instead. In general any score that is convex is ok, as it is then guaranteed to have a global optimum.\n",
    "\n",
    "Instead of using accuracy as our cross-validation metric, we'll use log likelihood instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2 Write a custom score function, log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using clf.predict_logproba, write a function that computes the log-likelihood of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "log_likelihood\n",
    "\n",
    "Compute the log likelihood of a dataset according to \n",
    "a Naive Bayes classifier. \n",
    "The Log Likelihood is defined by\n",
    "\n",
    "L = Sum_positive(logP(positive)) + Sum_negative(logP(negative))\n",
    "\n",
    "Where Sum_positive indicates a sum over all positive reviews, \n",
    "and Sum_negative indicates a sum over negative reviews\n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "clf : Naive Bayes classifier\n",
    "x : (nexample, nfeature) array\n",
    "    The input data\n",
    "y : (nexample) integer array\n",
    "    Whether each review is Fresh\n",
    "\"\"\"\n",
    "#your code here\n",
    "def log_likelihood(clf,x,y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's a function to estimate the cross-validated value of a scoring function, given a classifier and data. Why are we providing this function?\n",
    "\n",
    "Why not use `do_classify`? Indeed we could, but we also want to set up a parameter for the vectorization pre-processing step. We could use a pipeline for this purpose, and then use `do_classify`.\n",
    "\n",
    "Here we'll have you implement the pipeline yourself, since it has two steps. This way the process of setting up a pipeline becomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_score(clf, x, y, score_func, nfold=5):\n",
    "    \"\"\"\n",
    "    Uses 5-fold cross validation to estimate a score of a classifier\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    clf : Classifier object\n",
    "    x : Input feature vector\n",
    "    y : Input class labels\n",
    "    score_func : Function like log_likelihood, that takes (clf, x, y) as input,\n",
    "                 and returns a score\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    The average score obtained by splitting (x, y) into 5 folds of training and \n",
    "    test sets, fitting on the training set, and evaluating score_func on the test set\n",
    "    \n",
    "    Examples\n",
    "    cv_score(clf, x, y, log_likelihood)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf, x[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Calibration of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code is simply lifted from cs109 2013 hw 3 and labs 9 and 10. You always want to check the calibration of a probabilistic classifier:  -- that is, whether a prediction made with a confidence of x% is correct approximately x% of the time. We might say a model is over-confident if the positive fraction is usually closer to 0.5 than expected (that is, there is more uncertainty than the model predicted). Likewise, a model is under-confident if the probabilities are usually further away from 0.5. Here we provide code to make a calibration plot. See Lab 10 for a description of what this code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical P(+)\")\n",
    "    \n",
    "    #the distribution of P(+)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    \n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    \n",
    "    plt.xlabel(\"Predicted P(+)\")\n",
    "    plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are going to use a simple `CountVectorizer` here, along with a Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.3 Write a custom cross-validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write a cross-validation loop that loops over the `min_df` parameter from `CountVectorizer` and `alpha` from `MultinomialNB`. \n",
    "\n",
    "- Use our adjective vocabulary `adjvocab` as an argument to `sklearn`'s CountVectorizer. This guarantees that the indices we set up will be used.\n",
    "- note that `min_df` affects this vectorization, and thus the vectorizer is created afresh at each point on the cross-validation grid.\n",
    "- `make_xy` thus needs to run for each hyper-parameter combination as well since the features are just slightly different. We use the same mask to make sure we are getting the same training samples\n",
    "- the cross-validation happens inside the `cv_score` function. Think of why this is ok...shouldnt feature creation happen inside the cross-validation loop? (answer this below your code)\n",
    "\n",
    "Output the best `alpha` in the variable `best_alpha` and `best_min_df` in the variable `best_min_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "alphas = [0, .1, 1, 5, 10, 50,100,500]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(vocabulary=adjvocab, min_df = min_df)       \n",
    "        Xthis, ythis = make_xy(X,y, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        #your code here\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.000000\n",
      "min_df: 0.000010\n"
     ]
    }
   ],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*your answer here* <br>\n",
    "When we use accuracy for the score, we should worry about overfitting, and we can examine overfitting by testing our model on the data set that is separate to the training set. Here the score is likelihood, not accuracy, so we don't have to worry about overfitting, which is one of the greatest advantage of using Bayes method. Therefore, we don't really have to use the same validation set over the loop in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.4 Test time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you've determined values for alpha and min_df that optimize the cross-validated log-likelihood, repeat the CountVectorization and fitting to train a final classifier with these parameters, and draw calibration plots on the test set. Calculate the accuracy on the training and testing set. Comment on your results and the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.75\n",
      "Accuracy on test data:     0.72\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAykAAAIyCAYAAADPDIWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f8052d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code here\n",
    "vectorizer = CountVectorizer(vocabulary=adjvocab,min_df=best_min_df)\n",
    "Xthis, ythis = make_xy(X,y, vectorizer)\n",
    "xtrain=Xthis[mask]\n",
    "ytrain=ythis[mask]\n",
    "xtest=Xthis[~mask]\n",
    "ytest=ythis[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "# Your code here. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "\n",
    "calibration_plot(clf,xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*your answer here* <br> According to the plot, the prediction overestimates when the word is very 'fresh', while it tends to underestimate when the word is 'rotten'. These two local biases cancel out, so overall, there is not much bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Putting topics and sentiment analysis together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have probabilities for which topic a sentence belongs to based on the nouns, and we have our Naive Bayes classifier which predicts the sentiment of a whole review based on its adjectives. We need to modify our sentiment analysis to work on the level of single sentences, like we did for the topic modeling. The basic idea is summed up in this diagram:\n",
    "\n",
    "<img src=\"process.jpg\" width=800 height=600/>\n",
    "\n",
    "which shows a bunch of sentiments for the topics T0 and T1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive Bayes Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our naive Bayes classifier computes $P(\\text{class}\\,|\\,\\text{review})$, where the _class_ is \"whether the review is positive or negative\" and the _review_ corresponds to a set of adjectives $\\{\\text{word}_1, \\text{word}_2, \\text{word}_3, ...\\}$. \n",
    "\n",
    "According to Bayes theoreme we have:\n",
    "$$P(\\text{class}\\,|\\,\\text{word}_1, \\text{word}_2, ...) = \\frac{P(\\text{word}_1. \\text{word}_2, ...\\,|\\,\\text{class})P(\\text{class})}{P(\\text{word}_1, \\text{word}_2, ...)}$$\n",
    "\n",
    "Remember that the Naive Bayes classifier assumes conditional independence, i.e. it assumes that \n",
    "\n",
    "$P(\\text{word}_1, \\text{word}_2, ... \\,|\\, \\text{class}) = P(\\text{word_1}\\,|\\,\\text{class}) \\cdot P(\\text{word_2}\\,|\\,\\text{class}) \\cdot ...$. \n",
    "\n",
    "This means that during training our classifier estimated $P(\\text{word}_i \\,|\\, \\text{class})$, so it is pretty straightforward to apply it to the words from single sentences instead of whole reviews. We have to be careful though, because sentences typically are much shorter than whole reviews this means that we need to re-calibrate our conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.1 Obtain log-probabilities for the adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to compute all the class-conditional probabilities $P(\\text{word}_i \\,|\\, \\text{class})$ for every word in our adjective vocabulary. Use the vectorizer's `get_feature_names` function and the Naive Bayes classifier's `feature_log_prob_` (see the docs for `MultinomialNB`) to create two dictionaries.  One dictionary is for $\\log P(\\text{word}_i \\,|\\, \\text{class}=1)$ and the other is for $\\log P(\\text{word}_i \\,|\\, \\text{class}=0)$. Call the first dictionary `logpositives` and the second one `lognegatives` respectively. Thus the keys are the words themselves and the values are the log-probabilities\n",
    "\n",
    "Note that we are using the logarithm here. As we multiply probabilities, we can run into numerical problems if we have too many factors. In log space multiplications become sums and our computations is more stable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "vectorizer = CountVectorizer(vocabulary=adjvocab,min_df=best_min_df)\n",
    "Xthis, ythis = make_xy(X,y, vectorizer)\n",
    "xtrain=Xthis[mask]\n",
    "ytrain=ythis[mask]\n",
    "xtest=Xthis[~mask]\n",
    "ytest=ythis[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature = vectorizer.get_feature_names()\n",
    "log_prob = clf.feature_log_prob_\n",
    "\n",
    "logpositives = {feature:log_prob for feature, log_prob in zip(feature,log_prob[1])}\n",
    "lognegatives = {feature:log_prob for feature, log_prob in zip(feature,log_prob[0])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.2 Write a function to compute the probability that a sentence is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We've calculated all the $P(\\text{word}_i \\,|\\, \\text{class}=1)$ and $P(\\text{word}_i \\,|\\, \\text{class}=0)$ above as logs. This has been done for all the adjectives in each review.\n",
    "\n",
    "Now we wish to invert the question. We wish to ask: given a sentence $s$ of adjectives (since we stripped the nouns), whats the probability that this sentence is positive? \n",
    "\n",
    "We use Bayes theorem for this (+ is the class):\n",
    "\n",
    "$$P(+\\,|\\,s) = \\frac{P(s\\,|\\,+)P(+)}{P(s\\,|\\,+)P(+) + P(s\\,|\\,-)P(-)}$$\n",
    "\n",
    "Using the Naive Bayes assumption we find for the adjectives $\\text{word}_i$:\n",
    "\n",
    "$$P(s\\,|\\,+) = \\prod_i P(\\text{word}_i\\,|\\,+) \\implies log(P(s\\,|\\,+)) = \\sum_i log(P(\\text{word}_i\\,|\\,+))$$\n",
    "\n",
    "Use these two formulae to write a function `calc_pplus` which takes 5 arguments: a list of adjectives `adjlist` constituting a sentence, a dictionary of logpositives `lp`, a dictionary of lognegatives `ln`, the prior probability of a review being positive `pp`, and the prior probability of a review being negative `pn`; and outputs $P(s\\,|\\,+)$. (The prior probabilities were calculated earlier in this notebook)\n",
    "\n",
    "Here is the spec:\n",
    "`def calc_pplus(adjlist, lp, ln, pp,pn)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "def calc_pplus(adjlist,lp,ln,pp,pn):\n",
    "    logpsplus = np.array([lp[adj] for adj in adjlist]).sum()\n",
    "    logpsminus = np.array([ln[adj] for adj in adjlist]).sum()\n",
    "    return np.exp(logpsplus)*pp/(np.exp(logpsplus)*pp+np.exp(logpsminus)*pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Compute the topic for a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Back to our LDA topics. First we do a sanity check. We obtain all the review ids, and check that the length of the `parseout` array is identical to that of the `reviews` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews=subdf.map(lambda r: r.review_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91336, 91336)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parseout),len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We write a function `choose_topic` which chooses which of the two LDA topics (clusters) we ought to pick for a sentence, given the bag of words for that sentence. This simply uses the higher probability cluster to make a choice. Sometimes `get_document_topics` only outputs one cluster if the probability is overwhelmingly high. So we need to handle this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def choose_topic(ldamodel, bow):\n",
    "    tee = lda2.get_document_topics(bow)\n",
    "    if len(tee)==2:\n",
    "        t1,t2=tee\n",
    "        if t2[1] >= t1[1]:#get higher probability topic\n",
    "            topicis=t2[0]\n",
    "        else:\n",
    "            topicis=t1[0]\n",
    "    elif len(tee)==1:#if only one was provided its very high probability. Take it\n",
    "        teetuple=tee[0]\n",
    "        topicis=teetuple[0]\n",
    "    return topicis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now combine the functions `calc_pplus` and `choose_topic` in the loop below to obtain the sentence topic and calculate the sentiment of the sentence. Note how we use the length of the noun-list for each review from `parseout` to obtain the appropriate bag-of-words from our corpus. We store our output for each review in a dictionary with keys review-id and values a list of dictionaries, one for each sentence with the topic `topic` of the sentence and the probability `pplus` of it \"being\" or predicting positive. This loop will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter=0\n",
    "reviewdict={}\n",
    "for i, rid in enumerate(reviews):\n",
    "    rlist=[]\n",
    "    nlist, alist = parseout[i]\n",
    "    ln=len(nlist)\n",
    "    localbow=corpus[counter:counter+ln]\n",
    "    for bow, adj, noun in zip(localbow, alist, nlist):\n",
    "        doc=\" \".join([id2word[e[0]] for e in bow])\n",
    "        pplus=calc_pplus(adj, logpositives, lognegatives, priorp, priorn)\n",
    "        topicis=choose_topic(lda2, bow)\n",
    "        ldict={\"topic\": topicis, 'pplus':pplus}\n",
    "        rlist.append(ldict)\n",
    "    reviewdict[rid]=rlist\n",
    "    counter=counter+ln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  Create a dataframe with all of this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now write some Spark code to combine this dictionary with the original `subdf` dataframe to get a dataframe `completedf` which adds in the individual sentences for each review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The function `extendit` takes a row from `subdf`, gets the `review_id`, uses it to lookup the `reviewdict` above, finds the list of sentence dictionaries there, makes a copy of the dictionaries, and augments them with information from `subdf`. So our output is a list of dicts, one for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extendit(row):\n",
    "    outlist=[]\n",
    "    revs=reviewdict[row.review_id]\n",
    "    for r in revs:\n",
    "        r2=r.copy()\n",
    "        r2['business_id']=row.business_id\n",
    "        r2['user_id']=row.user_id\n",
    "        r2['review_id']=row.review_id\n",
    "        r2['stars']=row.stars\n",
    "        r2['user_avg']=row.user_avg\n",
    "        outlist.append(r2)\n",
    "    return outlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use Spark `flatMap` to create a huge list of dictionaries, one per sentence in the review, and then combine these into a Pandas (_not_ Spark) dataframe `completedf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 s, sys: 2.29 s, total: 33.3 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "newdfin=subdf.flatMap(lambda r: extendit(r)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'business_id': u'KayYbHCt-RkbGcPdGOThNg',\n",
       "  'pplus': 0.56680675094281785,\n",
       "  'review_id': u'2xCds3bp0wM6nHMz3H65vg',\n",
       "  'stars': 4,\n",
       "  'topic': 0,\n",
       "  'user_avg': 3.6,\n",
       "  'user_id': u'4-3IU5uUH90m21TWbZhhnA'},\n",
       " {'business_id': u'KayYbHCt-RkbGcPdGOThNg',\n",
       "  'pplus': 0.64807039132888733,\n",
       "  'review_id': u'2xCds3bp0wM6nHMz3H65vg',\n",
       "  'stars': 4,\n",
       "  'topic': 0,\n",
       "  'user_avg': 3.6,\n",
       "  'user_id': u'4-3IU5uUH90m21TWbZhhnA'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdfin.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 289 ms, total: 1.51 s\n",
      "Wall time: 58.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_dicts=newdfin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>pplus</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>topic</th>\n",
       "      <th>user_avg</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>2xCds3bp0wM6nHMz3H65vg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4-3IU5uUH90m21TWbZhhnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>0.648070</td>\n",
       "      <td>2xCds3bp0wM6nHMz3H65vg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4-3IU5uUH90m21TWbZhhnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>0.836425</td>\n",
       "      <td>2xCds3bp0wM6nHMz3H65vg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4-3IU5uUH90m21TWbZhhnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>0.782402</td>\n",
       "      <td>2xCds3bp0wM6nHMz3H65vg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4-3IU5uUH90m21TWbZhhnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>0.506715</td>\n",
       "      <td>2xCds3bp0wM6nHMz3H65vg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4-3IU5uUH90m21TWbZhhnA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id     pplus               review_id  stars  topic  user_avg                 user_id\n",
       "0  KayYbHCt-RkbGcPdGOThNg  0.566807  2xCds3bp0wM6nHMz3H65vg      4      0       3.6  4-3IU5uUH90m21TWbZhhnA\n",
       "1  KayYbHCt-RkbGcPdGOThNg  0.648070  2xCds3bp0wM6nHMz3H65vg      4      0       3.6  4-3IU5uUH90m21TWbZhhnA\n",
       "2  KayYbHCt-RkbGcPdGOThNg  0.836425  2xCds3bp0wM6nHMz3H65vg      4      0       3.6  4-3IU5uUH90m21TWbZhhnA\n",
       "3  KayYbHCt-RkbGcPdGOThNg  0.782402  2xCds3bp0wM6nHMz3H65vg      4      0       3.6  4-3IU5uUH90m21TWbZhhnA\n",
       "4  KayYbHCt-RkbGcPdGOThNg  0.506715  2xCds3bp0wM6nHMz3H65vg      4      1       3.6  4-3IU5uUH90m21TWbZhhnA"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completedf=pd.DataFrame(list_of_dicts)\n",
    "completedf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see this is a bigger dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462206, 7)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completedf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.3 Get the stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now going to group by `review_id` and `topic` to calculate statistics on all the sentences that belong to a particular review, and to a particular topic: \n",
    "\n",
    "`completedf.groupby(['review_id', 'topic'])`\n",
    "\n",
    "For each group in this group-by we will apply a function `get_stats` which takes the group as an argument and returns a one-row dataframe with columns:\n",
    "\n",
    "- `min`: minimum value of column `pplus`\n",
    "- `max`: maximum value of column `pplus`\n",
    "- `rid`: restaurant_id for the review\n",
    "- `uavg`: average user rating for user doing this review\n",
    "- `count`: number of sentences\n",
    "- `var`: variance of column `pplus` calculated with `ddof=1`. If the sample variance is NAN when there is only 1 sentence in a review (which happens quite often) set the variance to 0\n",
    "- `mean`: the mean of the column `pplus`\n",
    "- `stars`: the number of stars for the review.\n",
    "\n",
    "Set the index of this one line dataframe to the restaurant id (`rid`).\n",
    "\n",
    "What this function does is that it gives us review and topic specific statistics, computed on the sentences in the review.\n",
    "\n",
    "The spec of this function is:\n",
    "\n",
    "`def get_stats(group)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "def get_stats(group):\n",
    "    var = np.var(group.pplus,ddof=1)\n",
    "    if np.isnan(var):\n",
    "        var = 0\n",
    "    \n",
    "    df= pd.DataFrame({'min':np.min(group.pplus),'max':np.max(group.pplus),'rid':np.unique(group.business_id),\n",
    "                        'uavg':np.unique(group.user_avg),'count':group.business_id.count(),'var':var,\n",
    "                         'mean':np.mean(group.pplus),'stars':np.unique(group.stars)})\n",
    "    return df.set_index('rid')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now run the code on the `completedf`. The group-by will automatically concatenate these one-row dataframes for us. This is a slow function, taking about 5 mins on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 54s, sys: 29.1 s, total: 7min 23s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftouse=completedf.groupby(['review_id', 'topic']).apply(get_stats).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see the shape of `dftouse` and what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148355, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>rid</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>stars</th>\n",
       "      <th>uavg</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---gFan7_-hicaSgAi86Hg</td>\n",
       "      <td>0</td>\n",
       "      <td>btay-zbv6GGEp6c3Wg2MSw</td>\n",
       "      <td>2</td>\n",
       "      <td>0.610093</td>\n",
       "      <td>0.579968</td>\n",
       "      <td>0.549844</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.001815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---gFan7_-hicaSgAi86Hg</td>\n",
       "      <td>1</td>\n",
       "      <td>btay-zbv6GGEp6c3Wg2MSw</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668355</td>\n",
       "      <td>0.668355</td>\n",
       "      <td>0.668355</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---sV8KdwfBoDw38KW_WnQ</td>\n",
       "      <td>0</td>\n",
       "      <td>VgLiSW1iGkpzIEXOgvUBEw</td>\n",
       "      <td>4</td>\n",
       "      <td>0.684918</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.238338</td>\n",
       "      <td>3</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>0.044633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---sV8KdwfBoDw38KW_WnQ</td>\n",
       "      <td>1</td>\n",
       "      <td>VgLiSW1iGkpzIEXOgvUBEw</td>\n",
       "      <td>6</td>\n",
       "      <td>0.825936</td>\n",
       "      <td>0.587913</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>3</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>0.022411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--0MzHNy7MVBRvZCOAeRPg</td>\n",
       "      <td>0</td>\n",
       "      <td>4gLecengX1JeGILm7DwU3w</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736145</td>\n",
       "      <td>0.736145</td>\n",
       "      <td>0.736145</td>\n",
       "      <td>5</td>\n",
       "      <td>3.829268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  topic                     rid  count       max      mean       min  stars      uavg       var\n",
       "0  ---gFan7_-hicaSgAi86Hg      0  btay-zbv6GGEp6c3Wg2MSw      2  0.610093  0.579968  0.549844      3  3.333333  0.001815\n",
       "1  ---gFan7_-hicaSgAi86Hg      1  btay-zbv6GGEp6c3Wg2MSw      1  0.668355  0.668355  0.668355      3  3.333333  0.000000\n",
       "2  ---sV8KdwfBoDw38KW_WnQ      0  VgLiSW1iGkpzIEXOgvUBEw      4  0.684918  0.551414  0.238338      3  3.285714  0.044633\n",
       "3  ---sV8KdwfBoDw38KW_WnQ      1  VgLiSW1iGkpzIEXOgvUBEw      6  0.825936  0.587913  0.397541      3  3.285714  0.022411\n",
       "4  --0MzHNy7MVBRvZCOAeRPg      0  4gLecengX1JeGILm7DwU3w      1  0.736145  0.736145  0.736145      5  3.829268  0.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print dftouse.shape\n",
    "dftouse.head()#note not all reviews will have both topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And finally we save it. This ends the Spark part of the homework. Save the csv file, store it somewhere safe first, and only then shut down your Vagrant/AWS cluster/Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dftouse.to_csv(\"dftouse.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Include `dftouse.csv` in your homework submission. Q5 is in `hw5part2.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
